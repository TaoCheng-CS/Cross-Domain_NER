# config file for train cross ner model

# supervised transfer dataset #
supervised_ner_1_train=data/conll03NER/train
supervised_ner_1_dev=data/conll03NER/dev
supervised_ner_1_test=data/conll03NER/test

supervised_ner_2_train=data/BioNLP13CG/train
supervised_ner_2_dev=data/BioNLP13CG/dev
supervised_ner_2_test=data/BioNLP13CG/test

supervised_lm_1_train=data/news.lm
supervised_lm_2_train=data/bio.lm

# unsupervised transfer dataset #
transfer_ner_1_train=data/conll03NER/train
transfer_ner_1_dev=data/conll03NER/dev
transfer_ner_1_test=data/conll03NER/test

transfer_ner_2_dev=data/news_tech/tech.test
transfer_ner_2_test=data/news_tech/tech.test

transfer_lm_1_train=data/news.lm
transfer_lm_2_train=data/tech.lm

# check point #
model_dir=data/check_point/cross_ner_model
init_dir=data/check_point/cross_ner_init

# word embed vocab:
word_embed_dir=data/vocab/glove.6B.100d.txt

# normalize
norm_word_emb= #False default
norm_char_emb= #False default
number_normalized= #True default

# network parameter #
MAX_SENTENCE_LENGTH= #128 default
MAX_WORD_LENGTH= #16 default

seg= #True/False for F1-scale/accuracy-scale
word_emb_dim= #word embedding size, 100 defualt
char_emb_dim= #character embedding size, 30 default
task_emb_dim= #task embedding size, 8 defualt
domain_emb_dim= #domain embedding size, 8 default

cnn_layer= #CNN word hidden layer size, 4 default
char_hidden_dim= #charater CNN/LSTM hidden size, 50 default
hidden_dim= #word LSTM/CNN hidden size, 200 default
dropout= # dropout rate, 0.5 default
lstm_layer= #word LSTM layer size, 1 default
bilstm= #True/False for using word bidirectional/unidirectional word LSTM

use_ner_crf= #True/False using crf/softmax for NER classify layer
use_char= #True/False for using/not using charater embedding

char_seq_feature= #CNN/LSTM for character feature extraction

# training parameter #
status= #train/decode
mode= #/transfer for supervised/unsupervised scenario
optimizer= #SGD/RMSPROP/ADAM
iteration= #100 default
batch_size= #judging by time cost
ave_batch_loss= #True default

learning_rate= #according to the optimizer
learning_rate_cpg= #learning for the parameter generation network
lr_decay= #0.05 default for SGD
momentum= #0/1 for not using/using momentum in the optimizer
l2= #parameter normalization, 1e-8 default
gpu= #False/True
